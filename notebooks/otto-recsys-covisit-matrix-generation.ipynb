{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c6715d",
   "metadata": {
    "papermill": {
     "duration": 0.004906,
     "end_time": "2025-10-05T15:58:01.588100",
     "exception": false,
     "start_time": "2025-10-05T15:58:01.583194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Otto RecSys - Candidate ReRank Model - Co-visit generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4ceb7",
   "metadata": {
    "papermill": {
     "duration": 0.003853,
     "end_time": "2025-10-05T15:58:01.596142",
     "exception": false,
     "start_time": "2025-10-05T15:58:01.592289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Tổng quan Giải pháp: Mô hình Candidate-ReRank với Luật Tự chế**\n",
    "\n",
    "Notebook này triển khai một mô hình gợi ý theo kiến trúc 2 giai đoạn kinh điển: **Tạo Ứng viên (Candidate Generation)** và **Tái xếp hạng (Re-ranking)**. Điểm đặc biệt của giải pháp này là giai đoạn Re-ranking được thực hiện hoàn toàn bằng các **Luật Tự chế (Handcrafted Rules)** thay vì dùng một mô hình Machine Learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Giai đoạn 1: Tạo Ứng viên (Candidate Generation)**\n",
    "\n",
    "Mục tiêu của giai đoạn này là tạo ra một danh sách các sản phẩm tiềm năng (ứng viên) cho mỗi người dùng từ nhiều nguồn khác nhau. Để tăng tốc độ xử lý trên bộ dữ liệu lớn, notebook sử dụng thư viện **RAPIDS cuDF** để tính toán trên GPU.\n",
    "\n",
    "Các nguồn ứng viên chính bao gồm:\n",
    "\n",
    "1.  **Lịch sử Tương tác của Người dùng (User History):**\n",
    "    *   Gợi ý lại chính những sản phẩm mà người dùng đã từng click, thêm vào giỏ, hoặc mua. Đây là nguồn tín hiệu cá nhân hóa mạnh nhất.\n",
    "\n",
    "2.  **Các Sản phẩm Phổ biến nhất (Popular Items):**\n",
    "    *   Top 20 sản phẩm được `click` hoặc `order` nhiều nhất trong toàn bộ tập dữ liệu. Đây là nguồn gợi ý dự phòng, đặc biệt hữu ích cho các người dùng mới.\n",
    "\n",
    "3.  **Ma trận Co-visitation 1 (\"Carts/Orders\" - Có trọng số loại):**\n",
    "    *   **Mục tiêu:** Dự đoán các hành động có ý định mua hàng cao (`carts`, `orders`).\n",
    "    *   **Logic:** Trả lời câu hỏi: \"Nếu người dùng đã tương tác với sản phẩm A, họ có khả năng sẽ *thêm vào giỏ/mua* sản phẩm B nào nhất?\". Các hành động `cart` và `order` được gán trọng số cao hơn để nhấn mạnh tầm quan trọng.\n",
    "\n",
    "4.  **Ma trận Co-visitation 2 (\"Buy2Buy\"):**\n",
    "    *   **Mục tiêu:** Tìm các mối liên hệ mua hàng chất lượng cao nhất.\n",
    "    *   **Logic:** Trả lời câu hỏi: \"Nếu người dùng đã *mua* (hoặc thêm vào giỏ) sản phẩm A, họ có khả năng sẽ *mua* (hoặc thêm vào giỏ) sản phẩm B nào nhất?\". Ma trận này hoàn toàn bỏ qua các hành động `click` để giảm nhiễu.\n",
    "\n",
    "5.  **Ma trận Co-visitation 3 (\"Clicks\" - Có trọng số thời gian):**\n",
    "    *   **Mục tiêu:** Dự đoán các hành động khám phá (`clicks`) và nắm bắt xu hướng.\n",
    "    *   **Logic:** Trả lời câu hỏi: \"Nếu người dùng đã tương tác với sản phẩm A, họ có khả năng sẽ *click* sản phẩm B nào nhất?\". Các tương tác xảy ra gần đây được gán trọng số cao hơn để ưu tiên các xu hướng mới.\n",
    "\n",
    "---\n",
    "\n",
    "### **Giai đoạn 2: Tái xếp hạng & Lựa chọn (Re-ranking)**\n",
    "\n",
    "Sau khi có danh sách ứng viên từ các nguồn trên, giai đoạn này sẽ sắp xếp lại chúng và chọn ra 20 gợi ý cuối cùng cho mỗi loại (`clicks`, `carts`, `orders`). Giai đoạn này được hiện thực hóa bằng hai hàm riêng biệt (`suggest_clicks` và `suggest_buys`) với các quy tắc ưu tiên thủ công:\n",
    "\n",
    "1.  **Tính gần đây (Recency):** Ưu tiên các sản phẩm trong lịch sử được người dùng tương tác gần nhất.\n",
    "2.  **Sự lặp lại (Repetition):** Các sản phẩm được người dùng tương tác nhiều lần sẽ được đẩy lên cao hơn.\n",
    "3.  **Ý định Mua hàng (Intent):** Các sản phẩm đã từng được `cart` hoặc `order` có độ ưu tiên cao.\n",
    "4.  **Tín hiệu Chất lượng cao:** Các gợi ý đến từ ma trận \"Buy2Buy\" được ưu tiên trong việc dự đoán `carts`/`orders`.\n",
    "5.  **Xử lý Đặc biệt:** Đối với những người dùng có lịch sử phong phú (tương tác với >= 20 sản phẩm), hệ thống sẽ chủ yếu dựa vào việc phân tích lịch sử của chính họ (kết hợp recency, repetition, và intent) thay vì dùng nhiều các gợi ý từ co-visitation.\n",
    "\n",
    "Đây là một baseline mạnh mẽ với phần tạo ứng viên rất chất lượng. Cơ hội cải tiến lớn nhất nằm ở việc thay thế các **Luật Tự chế** ở Giai đoạn 2 bằng một mô hình Machine Learning (ví dụ: LGBMRanker) để có thể \"học\" được các quy luật xếp hạng phức tạp hơn từ dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397712a0",
   "metadata": {
    "papermill": {
     "duration": 0.003746,
     "end_time": "2025-10-05T15:58:01.603805",
     "exception": false,
     "start_time": "2025-10-05T15:58:01.600059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Scripts\n",
    "Tạo ra các scripts/source code/functions tái sử dụng được, tuân thủ DRY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52061299",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:01.612280Z",
     "iopub.status.busy": "2025-10-05T15:58:01.612058Z",
     "iopub.status.idle": "2025-10-05T15:58:10.007340Z",
     "shell.execute_reply": "2025-10-05T15:58:10.006577Z"
    },
    "papermill": {
     "duration": 8.401185,
     "end_time": "2025-10-05T15:58:10.008822",
     "exception": false,
     "start_time": "2025-10-05T15:58:01.607637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import gc\n",
    "import glob\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cedc7a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.019634Z",
     "iopub.status.busy": "2025-10-05T15:58:10.018917Z",
     "iopub.status.idle": "2025-10-05T15:58:10.023016Z",
     "shell.execute_reply": "2025-10-05T15:58:10.022388Z"
    },
    "papermill": {
     "duration": 0.010892,
     "end_time": "2025-10-05T15:58:10.023991",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.013099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.02.02\n"
     ]
    }
   ],
   "source": [
    "print(cudf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fdcc033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.032962Z",
     "iopub.status.busy": "2025-10-05T15:58:10.032388Z",
     "iopub.status.idle": "2025-10-05T15:58:10.035452Z",
     "shell.execute_reply": "2025-10-05T15:58:10.034932Z"
    },
    "papermill": {
     "duration": 0.008505,
     "end_time": "2025-10-05T15:58:10.036475",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.027970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TYPE_LABELS_MAPPING = {\n",
    "    'clicks': 0,\n",
    "    'carts': 1,\n",
    "    'orders': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d37eb",
   "metadata": {
    "papermill": {
     "duration": 0.004308,
     "end_time": "2025-10-05T15:58:10.044725",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.040417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Cache file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76cbf4f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.053052Z",
     "iopub.status.busy": "2025-10-05T15:58:10.052879Z",
     "iopub.status.idle": "2025-10-05T15:58:10.056746Z",
     "shell.execute_reply": "2025-10-05T15:58:10.056279Z"
    },
    "papermill": {
     "duration": 0.009248,
     "end_time": "2025-10-05T15:58:10.057801",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.048553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_DATA_CACHE = dict()\n",
    "\n",
    "def read_file(file_path: str):\n",
    "    global _DATA_CACHE\n",
    "    return cudf.DataFrame(_DATA_CACHE[file_path])\n",
    "\n",
    "def load_file(file_path: str):\n",
    "    global _DATA_CACHE\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Reduce timestamp memory\n",
    "    df['ts'] = (df['ts'] / 1000).astype('int32')\n",
    "    df['type'] = df['type'].map(TYPE_LABELS_MAPPING).astype('int8')\n",
    "    _DATA_CACHE[file_path] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b21813",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.066710Z",
     "iopub.status.busy": "2025-10-05T15:58:10.066157Z",
     "iopub.status.idle": "2025-10-05T15:58:10.069337Z",
     "shell.execute_reply": "2025-10-05T15:58:10.068818Z"
    },
    "papermill": {
     "duration": 0.008625,
     "end_time": "2025-10-05T15:58:10.070353",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.061728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cache_files(files: list[str]):\n",
    "    for file in files:\n",
    "        load_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc64439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.078889Z",
     "iopub.status.busy": "2025-10-05T15:58:10.078690Z",
     "iopub.status.idle": "2025-10-05T15:58:10.081871Z",
     "shell.execute_reply": "2025-10-05T15:58:10.081220Z"
    },
    "papermill": {
     "duration": 0.008559,
     "end_time": "2025-10-05T15:58:10.082910",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.074351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_cache():\n",
    "    global _DATA_CACHE\n",
    "    _DATA_CACHE.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0a4c6",
   "metadata": {
    "papermill": {
     "duration": 0.003786,
     "end_time": "2025-10-05T15:58:10.090618",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.086832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Constants/Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "369845f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.099175Z",
     "iopub.status.busy": "2025-10-05T15:58:10.098984Z",
     "iopub.status.idle": "2025-10-05T15:58:10.102050Z",
     "shell.execute_reply": "2025-10-05T15:58:10.101549Z"
    },
    "papermill": {
     "duration": 0.008413,
     "end_time": "2025-10-05T15:58:10.103081",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.094668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "READ_CT = 5\n",
    "NUM_CHUNK = 6\n",
    "FIX_SIZE = 1.86e6\n",
    "\n",
    "CO_VISIT_MATRIX_OUT_PATH_TEMPLATE = '{output_dir}/top_{top_k}_{matrix}_{part}.pqt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876b9d7",
   "metadata": {
    "papermill": {
     "duration": 0.003795,
     "end_time": "2025-10-05T15:58:10.110794",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.106999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Pre-calculate co-visit matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922e609",
   "metadata": {
    "papermill": {
     "duration": 0.003651,
     "end_time": "2025-10-05T15:58:10.118365",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.114714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Cart-Order Co-visit Matrix: Type Weighted\n",
    "Tạo ra một ma trận co-visitation trả lời câu hỏi: \"Nếu một người dùng đã tương tác (click/cart/order) với sản phẩm aid_x, thì họ có khả năng sẽ thêm vào giỏ (cart) hoặc mua (order) sản phẩm aid_y nào nhất?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b60f44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.126952Z",
     "iopub.status.busy": "2025-10-05T15:58:10.126745Z",
     "iopub.status.idle": "2025-10-05T15:58:10.137205Z",
     "shell.execute_reply": "2025-10-05T15:58:10.136698Z"
    },
    "papermill": {
     "duration": 0.016043,
     "end_time": "2025-10-05T15:58:10.138188",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.122145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_carts_orders_matrix(\n",
    "    files: list[str],\n",
    "    disk_pieces: int,\n",
    "    last_hold_aids_each_session: int,\n",
    "    valid_time: int,\n",
    "    top_k: int,\n",
    "    type_weight: dict,\n",
    "    save_parquet: bool=False,\n",
    "    output_dir: str|None=None\n",
    ") -> dict:\n",
    "    piece_size = FIX_SIZE / disk_pieces\n",
    "    chunk_size = int(np.ceil(len(files) / NUM_CHUNK))\n",
    "    saved_files = []\n",
    "    for part in range(disk_pieces):\n",
    "        # Mỗi vòng to sẽ xử lý 1/4 dữ liệu\n",
    "        print('='*20)\n",
    "        print(f'### DISK PART {part + 1}')\n",
    "    \n",
    "        for chunk_idx in range(NUM_CHUNK):\n",
    "            # Qua từng chunk nhỏ để giảm RAM\n",
    "            start_file_idx = chunk_idx * chunk_size\n",
    "            end_file_idx = min((chunk_idx + 1) * chunk_size, len(files))\n",
    "            print(f'Processing from file {start_file_idx} to {end_file_idx} in group of {READ_CT}')\n",
    "    \n",
    "            for k in range(start_file_idx, end_file_idx, READ_CT):\n",
    "                # Inner chunk\n",
    "                # Đọc 5 file một lúc từ cache, đẩy lên GPU và nối lại\n",
    "                df = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k + i < end_file_idx: \n",
    "                        df.append(read_file(files[k+i]))\n",
    "                df = cudf.concat(df, ignore_index=True, axis=0)\n",
    "    \n",
    "                # Sắp xếp theo session và thời gian (giảm dần)\n",
    "                df = df.sort_values(['session','ts'], ascending=[True,False])\n",
    "                \n",
    "                # --- Tối ưu hóa quan trọng ---\n",
    "                # Chỉ giữ lại 30 hành động cuối cùng của mỗi session\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<last_hold_aids_each_session].drop('n',axis=1)\n",
    "    \n",
    "                # Tự merge một dataframe với chính nó dựa trên session\n",
    "                # Kết quả: tạo ra tất cả các cặp (aid_x, aid_y) có thể có trong cùng 1 session\n",
    "                df = df.merge(df, on='session')\n",
    "    \n",
    "                # --- Lọc các cặp không hợp lệ ---\n",
    "                # 1. Chỉ giữ các cặp có thời gian cách nhau không quá 1 ngày (24*60*60 giây)\n",
    "                # 2. Loại bỏ các cặp mà aid_x giống hệt aid_y\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs()< valid_time) & (df.aid_x != df.aid_y)]\n",
    "    \n",
    "                # Chỉ tính cho các aid_x thuộc phần hiện tại\n",
    "                df = df.loc[(df.aid_x >= part*piece_size) & (df.aid_x < (part+1)*piece_size)]\n",
    "    \n",
    "                # --- Gán trọng số và tính toán ---\n",
    "                # Chỉ giữ các cột cần thiết và loại bỏ các cặp trùng lặp\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                # Dùng .map() để gán trọng số dựa trên 'type' của sản phẩm thứ hai (aid_y)\n",
    "                df['wgt'] = df.type_y.map(type_weight)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "    \n",
    "                # Nhóm theo cặp (aid_x, aid_y) và cộng tất cả các trọng số lại\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "    \n",
    "                # Combine inner chunk\n",
    "                if k == start_file_idx:\n",
    "                    inner_tmp = df\n",
    "                else:\n",
    "                    inner_tmp = inner_tmp.add(df, fill_value=0)\n",
    "    \n",
    "                print(k,', ',end='')\n",
    "            print(\"=\"*10)\n",
    "            # Combine outer chunk\n",
    "            if start_file_idx == 0:\n",
    "                outer_tmp = inner_tmp\n",
    "            else:\n",
    "                outer_tmp = outer_tmp.add(inner_tmp, fill_value=0)\n",
    "            del inner_tmp, df\n",
    "            gc.collect()\n",
    "    \n",
    "        # Convert matrix to dict\n",
    "        outer_tmp = outer_tmp.reset_index()\n",
    "        outer_tmp = outer_tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "        outer_tmp = outer_tmp.reset_index(drop=True)\n",
    "        outer_tmp['n'] = outer_tmp.groupby('aid_x')['aid_y'].cumcount()\n",
    "        outer_tmp = outer_tmp.loc[outer_tmp.n < top_k].drop('n', axis=1)\n",
    "    \n",
    "        # Save to disk\n",
    "        if save_parquet:\n",
    "            out_path = CO_VISIT_MATRIX_OUT_PATH_TEMPLATE.format(\n",
    "                output_dir=output_dir if output_dir else '/kaggle/working',\n",
    "                top_k=top_k,\n",
    "                matrix='carts_orders',\n",
    "                part=part\n",
    "            )\n",
    "            outer_tmp.to_pandas().to_parquet(out_path, index=None)\n",
    "            saved_files.append(out_path)\n",
    "\n",
    "    return {\n",
    "        'num_part': len(saved_files),\n",
    "        'matrix_type': 'carts_orders',\n",
    "        'saved_files': saved_files,\n",
    "        'top_k': top_k\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c918d4c",
   "metadata": {
    "papermill": {
     "duration": 0.00375,
     "end_time": "2025-10-05T15:58:10.145905",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.142155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### BUY2BUY Co-visit Matrix\n",
    "Tạo ra một ma trận co-visitation trả lời câu hỏi: \"Nếu một người dùng đã thêm vào giỏ (cart) hoặc mua (order) sản phẩm aid_x, thì họ có khả năng sẽ thêm vào giỏ hoặc mua sản phẩm aid_y nào nhất?\".\n",
    "Đây là tín hiệu về ý định mua hàng nghiêm túc, vì nó hoàn toàn bỏ qua các hành động \"click\" vốn có thể rất nhiễu (người dùng có thể click vào nhiều thứ nhưng không có ý định mua)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bbfe30b",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.154780Z",
     "iopub.status.busy": "2025-10-05T15:58:10.154445Z",
     "iopub.status.idle": "2025-10-05T15:58:10.165307Z",
     "shell.execute_reply": "2025-10-05T15:58:10.164777Z"
    },
    "papermill": {
     "duration": 0.016437,
     "end_time": "2025-10-05T15:58:10.166230",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.149793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_buy2buy_matrix(\n",
    "    files: list[str],\n",
    "    disk_pieces: int,\n",
    "    last_hold_aids_each_session: int,\n",
    "    valid_time: int,\n",
    "    top_k: int,\n",
    "    save_parquet: bool=False,\n",
    "    output_dir: str|None=None\n",
    "):\n",
    "    piece_size = FIX_SIZE / disk_pieces\n",
    "    chunk_size = int(np.ceil(len(files) / NUM_CHUNK))\n",
    "    saved_files = []\n",
    "    for part in range(disk_pieces):\n",
    "        # Mỗi vòng to sẽ xử lý 1/4 dữ liệu\n",
    "        print('='*20)\n",
    "        print(f'### DISK PART {part + 1}')\n",
    "    \n",
    "        for chunk_idx in range(NUM_CHUNK):\n",
    "            # Qua từng chunk nhỏ để giảm RAM\n",
    "            start_file_idx = chunk_idx * chunk_size\n",
    "            end_file_idx = min((chunk_idx + 1) * chunk_size, len(files))\n",
    "            print(f'Processing from file {start_file_idx} to {end_file_idx} in group of {READ_CT}')\n",
    "    \n",
    "            for k in range(start_file_idx, end_file_idx, READ_CT):\n",
    "                # Inner chunk\n",
    "                # Đọc 5 file một lúc từ cache, đẩy lên GPU và nối lại\n",
    "                df = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k + i < end_file_idx: \n",
    "                        df.append(read_file(files[k+i]))\n",
    "                df = cudf.concat(df, ignore_index=True, axis=0)\n",
    "                df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "                # Sắp xếp theo session và thời gian (giảm dần)\n",
    "                df = df.sort_values(['session','ts'], ascending=[True,False])\n",
    "                \n",
    "                # --- Tối ưu hóa quan trọng ---\n",
    "                # Chỉ giữ lại 30 hành động cuối cùng của mỗi session\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<last_hold_aids_each_session].drop('n',axis=1)\n",
    "    \n",
    "                # Tự merge một dataframe với chính nó dựa trên session\n",
    "                # Kết quả: tạo ra tất cả các cặp (aid_x, aid_y) có thể có trong cùng 1 session\n",
    "                df = df.merge(df, on='session')\n",
    "    \n",
    "                # --- Lọc các cặp không hợp lệ ---\n",
    "                # 1. Chỉ giữ các cặp có thời gian cách nhau không quá 1 ngày (24*60*60 giây)\n",
    "                # 2. Loại bỏ các cặp mà aid_x giống hệt aid_y\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs()< valid_time) & (df.aid_x != df.aid_y)]\n",
    "    \n",
    "                # Chỉ tính cho các aid_x thuộc phần hiện tại\n",
    "                df = df.loc[(df.aid_x >= part*piece_size) & (df.aid_x < (part+1)*piece_size)]\n",
    "    \n",
    "                # --- Gán trọng số và tính toán ---\n",
    "                # Chỉ giữ các cột cần thiết và loại bỏ các cặp trùng lặp\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                # Dùng .map() để gán trọng số dựa trên 'type' của sản phẩm thứ hai (aid_y)\n",
    "                df['wgt'] = 1\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "    \n",
    "                # Nhóm theo cặp (aid_x, aid_y) và cộng tất cả các trọng số lại\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "    \n",
    "                # Combine inner chunk\n",
    "                if k == start_file_idx:\n",
    "                    inner_tmp = df\n",
    "                else:\n",
    "                    inner_tmp = inner_tmp.add(df, fill_value=0)\n",
    "    \n",
    "                print(k,', ',end='')\n",
    "            print(\"=\"*10)\n",
    "            # Combine outer chunk\n",
    "            if start_file_idx == 0:\n",
    "                outer_tmp = inner_tmp\n",
    "            else:\n",
    "                outer_tmp = outer_tmp.add(inner_tmp, fill_value=0)\n",
    "            del inner_tmp, df\n",
    "            gc.collect()\n",
    "    \n",
    "        # Convert matrix to dict\n",
    "        outer_tmp = outer_tmp.reset_index()\n",
    "        outer_tmp = outer_tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "        outer_tmp = outer_tmp.reset_index(drop=True)\n",
    "        outer_tmp['n'] = outer_tmp.groupby('aid_x')['aid_y'].cumcount()\n",
    "        outer_tmp = outer_tmp.loc[outer_tmp.n < top_k].drop('n', axis=1)\n",
    "    \n",
    "        # Save to disk\n",
    "        if save_parquet:\n",
    "            out_path = CO_VISIT_MATRIX_OUT_PATH_TEMPLATE.format(\n",
    "                output_dir=output_dir if output_dir else '/kaggle/working',\n",
    "                top_k=top_k,\n",
    "                matrix='buy2buy',\n",
    "                part=part\n",
    "            )\n",
    "            outer_tmp.to_pandas().to_parquet(out_path, index=None)\n",
    "            saved_files.append(out_path)\n",
    "\n",
    "    return {\n",
    "        'num_part': len(saved_files),\n",
    "        'matrix_type': 'buy2buy',\n",
    "        'saved_files': saved_files,\n",
    "        'top_k': top_k\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf72bc",
   "metadata": {
    "papermill": {
     "duration": 0.003685,
     "end_time": "2025-10-05T15:58:10.173825",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.170140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### \"Click\" Co-visit Matrix - Time Weighted\n",
    "Tạo ra một ma trận co-visitation trả lời câu hỏi: \"Nếu một người dùng đã tương tác (click/cart/order) với sản phẩm aid_x, thì họ có khả năng sẽ click vào sản phẩm aid_y nào nhất, với điều kiện là các tương tác gần đây phải quan trọng hơn?\".\n",
    "Ma trận này được gọi là \"Time Weighted\" vì trọng số của mỗi cặp tương tác phụ thuộc vào thời điểm nó xảy ra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "717c5454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.182395Z",
     "iopub.status.busy": "2025-10-05T15:58:10.182196Z",
     "iopub.status.idle": "2025-10-05T15:58:10.192500Z",
     "shell.execute_reply": "2025-10-05T15:58:10.192050Z"
    },
    "papermill": {
     "duration": 0.015959,
     "end_time": "2025-10-05T15:58:10.193568",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.177609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_clicks_matrix(\n",
    "    files: list[str],\n",
    "    disk_pieces: int,\n",
    "    last_hold_aids_each_session: int,\n",
    "    valid_time: int,\n",
    "    top_k: int,\n",
    "    time_weighted_func: callable,\n",
    "    save_parquet: bool=False,\n",
    "    output_dir: str|None=None\n",
    "):\n",
    "    piece_size = FIX_SIZE / disk_pieces\n",
    "    chunk_size = int(np.ceil(len(files) / NUM_CHUNK))\n",
    "    saved_files = []\n",
    "    for part in range(disk_pieces):\n",
    "        # Mỗi vòng to sẽ xử lý 1/4 dữ liệu\n",
    "        print('='*20)\n",
    "        print(f'### DISK PART {part + 1}')\n",
    "    \n",
    "        for chunk_idx in range(NUM_CHUNK):\n",
    "            # Qua từng chunk nhỏ để giảm RAM\n",
    "            start_file_idx = chunk_idx * chunk_size\n",
    "            end_file_idx = min((chunk_idx + 1) * chunk_size, len(files))\n",
    "            print(f'Processing from file {start_file_idx} to {end_file_idx} in group of {READ_CT}')\n",
    "    \n",
    "            for k in range(start_file_idx, end_file_idx, READ_CT):\n",
    "                # Inner chunk\n",
    "                # Đọc 5 file một lúc từ cache, đẩy lên GPU và nối lại\n",
    "                df = [read_file(files[k])]\n",
    "                for i in range(1, READ_CT):\n",
    "                    if k + i < end_file_idx: \n",
    "                        df.append(read_file(files[k+i]))\n",
    "                df = cudf.concat(df, ignore_index=True, axis=0)\n",
    "    \n",
    "                # Sắp xếp theo session và thời gian (giảm dần)\n",
    "                df = df.sort_values(['session','ts'], ascending=[True,False])\n",
    "                \n",
    "                # --- Tối ưu hóa quan trọng ---\n",
    "                # Chỉ giữ lại 30 hành động cuối cùng của mỗi session\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<last_hold_aids_each_session].drop('n',axis=1)\n",
    "    \n",
    "                # Tự merge một dataframe với chính nó dựa trên session\n",
    "                # Kết quả: tạo ra tất cả các cặp (aid_x, aid_y) có thể có trong cùng 1 session\n",
    "                df = df.merge(df, on='session')\n",
    "    \n",
    "                # --- Lọc các cặp không hợp lệ ---\n",
    "                # 1. Chỉ giữ các cặp có thời gian cách nhau không quá 1 ngày (24*60*60 giây)\n",
    "                # 2. Loại bỏ các cặp mà aid_x giống hệt aid_y\n",
    "                df = df.loc[((df.ts_x - df.ts_y).abs()< valid_time) & (df.aid_x != df.aid_y)]\n",
    "    \n",
    "                # Chỉ tính cho các aid_x thuộc phần hiện tại\n",
    "                df = df.loc[(df.aid_x >= part*piece_size) & (df.aid_x < (part+1)*piece_size)]\n",
    "    \n",
    "                # --- Gán trọng số và tính toán ---\n",
    "                # Chỉ giữ các cột cần thiết và loại bỏ các cặp trùng lặp\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y', 'ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = time_weighted_func(df['ts_x'])\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "    \n",
    "                # Nhóm theo cặp (aid_x, aid_y) và cộng tất cả các trọng số lại\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "    \n",
    "                # Combine inner chunk\n",
    "                if k == start_file_idx:\n",
    "                    inner_tmp = df\n",
    "                else:\n",
    "                    inner_tmp = inner_tmp.add(df, fill_value=0)\n",
    "    \n",
    "                print(k,', ',end='')\n",
    "            print(\"=\"*10)\n",
    "            # Combine outer chunk\n",
    "            if start_file_idx == 0:\n",
    "                outer_tmp = inner_tmp\n",
    "            else:\n",
    "                outer_tmp = outer_tmp.add(inner_tmp, fill_value=0)\n",
    "            del inner_tmp, df\n",
    "            gc.collect()\n",
    "    \n",
    "        # Convert matrix to dict\n",
    "        outer_tmp = outer_tmp.reset_index()\n",
    "        outer_tmp = outer_tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "        outer_tmp = outer_tmp.reset_index(drop=True)\n",
    "        outer_tmp['n'] = outer_tmp.groupby('aid_x')['aid_y'].cumcount()\n",
    "        outer_tmp = outer_tmp.loc[outer_tmp.n < top_k].drop('n', axis=1)\n",
    "    \n",
    "        # Save to disk\n",
    "        if save_parquet:\n",
    "            out_path = CO_VISIT_MATRIX_OUT_PATH_TEMPLATE.format(\n",
    "                output_dir=output_dir if output_dir else '/kaggle/working',\n",
    "                top_k=top_k,\n",
    "                matrix='clicks',\n",
    "                part=part\n",
    "            )\n",
    "            outer_tmp.to_pandas().to_parquet(out_path)\n",
    "            saved_files.append(out_path)\n",
    "\n",
    "    return {\n",
    "        'num_part': len(saved_files),\n",
    "        'matrix_type': 'clicks',\n",
    "        'saved_files': saved_files,\n",
    "        'top_k': top_k\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41ad0e",
   "metadata": {
    "papermill": {
     "duration": 0.003802,
     "end_time": "2025-10-05T15:58:10.201315",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.197513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2295725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.209851Z",
     "iopub.status.busy": "2025-10-05T15:58:10.209657Z",
     "iopub.status.idle": "2025-10-05T15:58:10.212443Z",
     "shell.execute_reply": "2025-10-05T15:58:10.211948Z"
    },
    "papermill": {
     "duration": 0.008254,
     "end_time": "2025-10-05T15:58:10.213441",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.205187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d380bc50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.222763Z",
     "iopub.status.busy": "2025-10-05T15:58:10.222239Z",
     "iopub.status.idle": "2025-10-05T15:58:10.226795Z",
     "shell.execute_reply": "2025-10-05T15:58:10.226238Z"
    },
    "papermill": {
     "duration": 0.009931,
     "end_time": "2025-10-05T15:58:10.227766",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.217835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def full_pipeline(\n",
    "    input_dir: str,\n",
    "    output_dir: str,\n",
    "    carts_orders_params: dict,\n",
    "    buy2buy_params: dict,\n",
    "    clicks_params: dict\n",
    "):\n",
    "    files = glob.glob(input_dir)\n",
    "    cache_files(files)\n",
    "\n",
    "    carts_orders_params.update({\n",
    "        'files': files,\n",
    "        'output_dir': output_dir\n",
    "    })\n",
    "\n",
    "    buy2buy_params.update({\n",
    "        'files': files,\n",
    "        'output_dir': output_dir\n",
    "    })\n",
    "\n",
    "    clicks_params.update({\n",
    "        'files': files,\n",
    "        'output_dir': output_dir\n",
    "    })\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    carts_orders_res = calculate_carts_orders_matrix(**carts_orders_params)\n",
    "    buy2buy_res = calculate_buy2buy_matrix(**buy2buy_params)\n",
    "    clicks_res = calculate_clicks_matrix(**clicks_params)\n",
    "\n",
    "    print(\"Result: \")\n",
    "    print(carts_orders_res)\n",
    "    print(buy2buy_res)\n",
    "    print(clicks_res)\n",
    "\n",
    "    clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbb7d2",
   "metadata": {
    "papermill": {
     "duration": 0.003781,
     "end_time": "2025-10-05T15:58:10.235389",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.231608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare 3 co-visit matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "702f7bbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.243924Z",
     "iopub.status.busy": "2025-10-05T15:58:10.243743Z",
     "iopub.status.idle": "2025-10-05T15:58:10.248655Z",
     "shell.execute_reply": "2025-10-05T15:58:10.248008Z"
    },
    "papermill": {
     "duration": 0.010293,
     "end_time": "2025-10-05T15:58:10.249644",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.239351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "carts_orders_params = {\n",
    "    'disk_pieces': 4,\n",
    "    'last_hold_aids_each_session': 30,\n",
    "    'valid_time': 24 * 60 * 60, # 1 ngày\n",
    "    'top_k': 15,\n",
    "    'type_weight': {0: 1, 1: 6, 2: 3}, # 0:clicks, 1:carts, 2:orders\n",
    "    'save_parquet': True,\n",
    "}\n",
    "\n",
    "buy2buy_params = {\n",
    "    'disk_pieces': 1,\n",
    "    'last_hold_aids_each_session': 30,\n",
    "    'valid_time': 14 * 24 * 60 * 60, # 14 ngày\n",
    "    'top_k': 15,\n",
    "    'save_parquet': True,\n",
    "}\n",
    "\n",
    "from datetime import datetime\n",
    "start_date = int(datetime(2022, 7, 31, 22, 0, 0).timestamp())\n",
    "end_date = int(datetime(2022, 9, 4, 22, 0, 0).timestamp())\n",
    "\n",
    "def linear_time_weighted(x):\n",
    "    return 1 + 3 * (x - start_date) / (end_date - start_date)\n",
    "    \n",
    "clicks_params = {\n",
    "    'disk_pieces': 4,\n",
    "    'last_hold_aids_each_session': 30,\n",
    "    'valid_time': 24 * 60 * 60, # 1 ngày\n",
    "    'top_k': 20,\n",
    "    'time_weighted_func': linear_time_weighted,\n",
    "    'save_parquet': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a8991",
   "metadata": {
    "papermill": {
     "duration": 0.003748,
     "end_time": "2025-10-05T15:58:10.257263",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.253515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### For submission pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5df463c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:58:10.265851Z",
     "iopub.status.busy": "2025-10-05T15:58:10.265660Z",
     "iopub.status.idle": "2025-10-05T16:07:37.665888Z",
     "shell.execute_reply": "2025-10-05T16:07:37.664938Z"
    },
    "papermill": {
     "duration": 567.40588,
     "end_time": "2025-10-05T16:07:37.667090",
     "exception": false,
     "start_time": "2025-10-05T15:58:10.261210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 22 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 22 to 44 in group of 5\n",
      "22 , 27 , 32 , 37 , 42 , ==========\n",
      "Processing from file 44 to 66 in group of 5\n",
      "44 , 49 , 54 , 59 , 64 , ==========\n",
      "Processing from file 66 to 88 in group of 5\n",
      "66 , 71 , 76 , 81 , 86 , ==========\n",
      "Processing from file 88 to 110 in group of 5\n",
      "88 , 93 , 98 , 103 , 108 , ==========\n",
      "Processing from file 110 to 129 in group of 5\n",
      "110 , 115 , 120 , 125 , ==========\n",
      "====================\n",
      "### DISK PART 2\n",
      "Processing from file 0 to 22 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 22 to 44 in group of 5\n",
      "22 , 27 , 32 , 37 , 42 , ==========\n",
      "Processing from file 44 to 66 in group of 5\n",
      "44 , 49 , 54 , 59 , 64 , ==========\n",
      "Processing from file 66 to 88 in group of 5\n",
      "66 , 71 , 76 , 81 , 86 , ==========\n",
      "Processing from file 88 to 110 in group of 5\n",
      "88 , 93 , 98 , 103 , 108 , ==========\n",
      "Processing from file 110 to 129 in group of 5\n",
      "110 , 115 , 120 , 125 , ==========\n",
      "====================\n",
      "### DISK PART 3\n",
      "Processing from file 0 to 22 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 22 to 44 in group of 5\n",
      "22 , 27 , 32 , 37 , 42 , ==========\n",
      "Processing from file 44 to 66 in group of 5\n",
      "44 , 49 , 54 , 59 , 64 , ==========\n",
      "Processing from file 66 to 88 in group of 5\n",
      "66 , 71 , 76 , 81 , 86 , ==========\n",
      "Processing from file 88 to 110 in group of 5\n",
      "88 , 93 , 98 , 103 , 108 , ==========\n",
      "Processing from file 110 to 129 in group of 5\n",
      "110 , 115 , 120 , 125 , ==========\n",
      "====================\n",
      "### DISK PART 4\n",
      "Processing from file 0 to 22 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 22 to 44 in group of 5\n",
      "22 , 27 , 32 , 37 , 42 , ==========\n",
      "Processing from file 44 to 66 in group of 5\n",
      "44 , 49 , 54 , 59 , 64 , ==========\n",
      "Processing from file 66 to 88 in group of 5\n",
      "66 , 71 , 76 , 81 , 86 , ==========\n",
      "Processing from file 88 to 110 in group of 5\n",
      "88 , 93 , 98 , 103 , 108 , ==========\n",
      "Processing from file 110 to 129 in group of 5\n",
      "110 , 115 , 120 , 125 , ==========\n",
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 22 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 22 to 44 in group of 5\n",
      "22 , 27 , 32 , 37 , 42 , ==========\n",
      "Processing from file 44 to 66 in group of 5\n",
      "44 , 49 , 54 , 59 , 64 , ==========\n",
      "Processing from file 66 to 88 in group of 5\n",
      "66 , 71 , 76 , 81 , 86 , ==========\n",
      "Processing from file 88 to 110 in group of 5\n",
      "88 , 93 , 98 , 103 , 108 , ==========\n",
      "Processing from file 110 to 129 in group of 5\n",
      "110 , 115 , 120 , 125 , ==========\n",
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 22 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 22 to 44 in group of 5\n",
      "22 , 27 , 32 , 37 , 42 , ==========\n",
      "Processing from file 44 to 66 in group of 5\n",
      "44 , 49 , 54 , 59 , 64 , ==========\n",
      "Processing from file 66 to 88 in group of 5\n",
      "66 , 71 , 76 , 81 , 86 , ==========\n",
      "Processing from file 88 to 110 in group of 5\n",
      "88 , 93 , 98 , 103 , 108 , ==========\n",
      "Processing from file 110 to 129 in group of 5\n",
      "110 , 115 , 120 , 125 , ==========\n",
      "====================\n",
      "### DISK PART 2\n",
      "Processing from file 0 to 22 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 22 to 44 in group of 5\n",
      "22 , 27 , 32 , 37 , 42 , ==========\n",
      "Processing from file 44 to 66 in group of 5\n",
      "44 , 49 , 54 , 59 , 64 , ==========\n",
      "Processing from file 66 to 88 in group of 5\n",
      "66 , 71 , 76 , 81 , 86 , ==========\n",
      "Processing from file 88 to 110 in group of 5\n",
      "88 , 93 , 98 , 103 , 108 , ==========\n",
      "Processing from file 110 to 129 in group of 5\n",
      "110 , 115 , 120 , 125 , ==========\n",
      "====================\n",
      "### DISK PART 3\n",
      "Processing from file 0 to 22 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 22 to 44 in group of 5\n",
      "22 , 27 , 32 , 37 , 42 , ==========\n",
      "Processing from file 44 to 66 in group of 5\n",
      "44 , 49 , 54 , 59 , 64 , ==========\n",
      "Processing from file 66 to 88 in group of 5\n",
      "66 , 71 , 76 , 81 , 86 , ==========\n",
      "Processing from file 88 to 110 in group of 5\n",
      "88 , 93 , 98 , 103 , 108 , ==========\n",
      "Processing from file 110 to 129 in group of 5\n",
      "110 , 115 , 120 , 125 , ==========\n",
      "====================\n",
      "### DISK PART 4\n",
      "Processing from file 0 to 22 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 22 to 44 in group of 5\n",
      "22 , 27 , 32 , 37 , 42 , ==========\n",
      "Processing from file 44 to 66 in group of 5\n",
      "44 , 49 , 54 , 59 , 64 , ==========\n",
      "Processing from file 66 to 88 in group of 5\n",
      "66 , 71 , 76 , 81 , 86 , ==========\n",
      "Processing from file 88 to 110 in group of 5\n",
      "88 , 93 , 98 , 103 , 108 , ==========\n",
      "Processing from file 110 to 129 in group of 5\n",
      "110 , 115 , 120 , 125 , ==========\n",
      "Result: \n",
      "{'num_part': 4, 'matrix_type': 'carts_orders', 'saved_files': ['/kaggle/working/submission_pipeline/train/top_15_carts_orders_0.pqt', '/kaggle/working/submission_pipeline/train/top_15_carts_orders_1.pqt', '/kaggle/working/submission_pipeline/train/top_15_carts_orders_2.pqt', '/kaggle/working/submission_pipeline/train/top_15_carts_orders_3.pqt'], 'top_k': 15}\n",
      "{'num_part': 1, 'matrix_type': 'buy2buy', 'saved_files': ['/kaggle/working/submission_pipeline/train/top_15_buy2buy_0.pqt'], 'top_k': 15}\n",
      "{'num_part': 4, 'matrix_type': 'clicks', 'saved_files': ['/kaggle/working/submission_pipeline/train/top_20_clicks_0.pqt', '/kaggle/working/submission_pipeline/train/top_20_clicks_1.pqt', '/kaggle/working/submission_pipeline/train/top_20_clicks_2.pqt', '/kaggle/working/submission_pipeline/train/top_20_clicks_3.pqt'], 'top_k': 20}\n",
      "CPU times: user 8min 17s, sys: 1min 10s, total: 9min 28s\n",
      "Wall time: 9min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "full_pipeline(\n",
    "    '/kaggle/input/otto-chunk-data-inparquet-format/train_parquet/*',\n",
    "    '/kaggle/working/submission_pipeline/train',\n",
    "    carts_orders_params,\n",
    "    buy2buy_params,\n",
    "    clicks_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cd3f561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T16:07:37.698637Z",
     "iopub.status.busy": "2025-10-05T16:07:37.698382Z",
     "iopub.status.idle": "2025-10-05T16:16:53.416499Z",
     "shell.execute_reply": "2025-10-05T16:16:53.415597Z"
    },
    "papermill": {
     "duration": 555.735148,
     "end_time": "2025-10-05T16:16:53.417767",
     "exception": false,
     "start_time": "2025-10-05T16:07:37.682619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 25 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 25 to 50 in group of 5\n",
      "25 , 30 , 35 , 40 , 45 , ==========\n",
      "Processing from file 50 to 75 in group of 5\n",
      "50 , 55 , 60 , 65 , 70 , ==========\n",
      "Processing from file 75 to 100 in group of 5\n",
      "75 , 80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 125 in group of 5\n",
      "100 , 105 , 110 , 115 , 120 , ==========\n",
      "Processing from file 125 to 146 in group of 5\n",
      "125 , 130 , 135 , 140 , 145 , ==========\n",
      "====================\n",
      "### DISK PART 2\n",
      "Processing from file 0 to 25 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 25 to 50 in group of 5\n",
      "25 , 30 , 35 , 40 , 45 , ==========\n",
      "Processing from file 50 to 75 in group of 5\n",
      "50 , 55 , 60 , 65 , 70 , ==========\n",
      "Processing from file 75 to 100 in group of 5\n",
      "75 , 80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 125 in group of 5\n",
      "100 , 105 , 110 , 115 , 120 , ==========\n",
      "Processing from file 125 to 146 in group of 5\n",
      "125 , 130 , 135 , 140 , 145 , ==========\n",
      "====================\n",
      "### DISK PART 3\n",
      "Processing from file 0 to 25 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 25 to 50 in group of 5\n",
      "25 , 30 , 35 , 40 , 45 , ==========\n",
      "Processing from file 50 to 75 in group of 5\n",
      "50 , 55 , 60 , 65 , 70 , ==========\n",
      "Processing from file 75 to 100 in group of 5\n",
      "75 , 80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 125 in group of 5\n",
      "100 , 105 , 110 , 115 , 120 , ==========\n",
      "Processing from file 125 to 146 in group of 5\n",
      "125 , 130 , 135 , 140 , 145 , ==========\n",
      "====================\n",
      "### DISK PART 4\n",
      "Processing from file 0 to 25 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 25 to 50 in group of 5\n",
      "25 , 30 , 35 , 40 , 45 , ==========\n",
      "Processing from file 50 to 75 in group of 5\n",
      "50 , 55 , 60 , 65 , 70 , ==========\n",
      "Processing from file 75 to 100 in group of 5\n",
      "75 , 80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 125 in group of 5\n",
      "100 , 105 , 110 , 115 , 120 , ==========\n",
      "Processing from file 125 to 146 in group of 5\n",
      "125 , 130 , 135 , 140 , 145 , ==========\n",
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 25 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 25 to 50 in group of 5\n",
      "25 , 30 , 35 , 40 , 45 , ==========\n",
      "Processing from file 50 to 75 in group of 5\n",
      "50 , 55 , 60 , 65 , 70 , ==========\n",
      "Processing from file 75 to 100 in group of 5\n",
      "75 , 80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 125 in group of 5\n",
      "100 , 105 , 110 , 115 , 120 , ==========\n",
      "Processing from file 125 to 146 in group of 5\n",
      "125 , 130 , 135 , 140 , 145 , ==========\n",
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 25 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 25 to 50 in group of 5\n",
      "25 , 30 , 35 , 40 , 45 , ==========\n",
      "Processing from file 50 to 75 in group of 5\n",
      "50 , 55 , 60 , 65 , 70 , ==========\n",
      "Processing from file 75 to 100 in group of 5\n",
      "75 , 80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 125 in group of 5\n",
      "100 , 105 , 110 , 115 , 120 , ==========\n",
      "Processing from file 125 to 146 in group of 5\n",
      "125 , 130 , 135 , 140 , 145 , ==========\n",
      "====================\n",
      "### DISK PART 2\n",
      "Processing from file 0 to 25 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 25 to 50 in group of 5\n",
      "25 , 30 , 35 , 40 , 45 , ==========\n",
      "Processing from file 50 to 75 in group of 5\n",
      "50 , 55 , 60 , 65 , 70 , ==========\n",
      "Processing from file 75 to 100 in group of 5\n",
      "75 , 80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 125 in group of 5\n",
      "100 , 105 , 110 , 115 , 120 , ==========\n",
      "Processing from file 125 to 146 in group of 5\n",
      "125 , 130 , 135 , 140 , 145 , ==========\n",
      "====================\n",
      "### DISK PART 3\n",
      "Processing from file 0 to 25 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 25 to 50 in group of 5\n",
      "25 , 30 , 35 , 40 , 45 , ==========\n",
      "Processing from file 50 to 75 in group of 5\n",
      "50 , 55 , 60 , 65 , 70 , ==========\n",
      "Processing from file 75 to 100 in group of 5\n",
      "75 , 80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 125 in group of 5\n",
      "100 , 105 , 110 , 115 , 120 , ==========\n",
      "Processing from file 125 to 146 in group of 5\n",
      "125 , 130 , 135 , 140 , 145 , ==========\n",
      "====================\n",
      "### DISK PART 4\n",
      "Processing from file 0 to 25 in group of 5\n",
      "0 , 5 , 10 , 15 , 20 , ==========\n",
      "Processing from file 25 to 50 in group of 5\n",
      "25 , 30 , 35 , 40 , 45 , ==========\n",
      "Processing from file 50 to 75 in group of 5\n",
      "50 , 55 , 60 , 65 , 70 , ==========\n",
      "Processing from file 75 to 100 in group of 5\n",
      "75 , 80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 125 in group of 5\n",
      "100 , 105 , 110 , 115 , 120 , ==========\n",
      "Processing from file 125 to 146 in group of 5\n",
      "125 , 130 , 135 , 140 , 145 , ==========\n",
      "Result: \n",
      "{'num_part': 4, 'matrix_type': 'carts_orders', 'saved_files': ['/kaggle/working/submission_pipeline/all/top_15_carts_orders_0.pqt', '/kaggle/working/submission_pipeline/all/top_15_carts_orders_1.pqt', '/kaggle/working/submission_pipeline/all/top_15_carts_orders_2.pqt', '/kaggle/working/submission_pipeline/all/top_15_carts_orders_3.pqt'], 'top_k': 15}\n",
      "{'num_part': 1, 'matrix_type': 'buy2buy', 'saved_files': ['/kaggle/working/submission_pipeline/all/top_15_buy2buy_0.pqt'], 'top_k': 15}\n",
      "{'num_part': 4, 'matrix_type': 'clicks', 'saved_files': ['/kaggle/working/submission_pipeline/all/top_20_clicks_0.pqt', '/kaggle/working/submission_pipeline/all/top_20_clicks_1.pqt', '/kaggle/working/submission_pipeline/all/top_20_clicks_2.pqt', '/kaggle/working/submission_pipeline/all/top_20_clicks_3.pqt'], 'top_k': 20}\n",
      "CPU times: user 8min 26s, sys: 1min 8s, total: 9min 35s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Full\n",
    "full_pipeline(\n",
    "    '/kaggle/input/otto-chunk-data-inparquet-format/*_parquet/*',\n",
    "    '/kaggle/working/submission_pipeline/all',\n",
    "    carts_orders_params,\n",
    "    buy2buy_params,\n",
    "    clicks_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ef97c",
   "metadata": {
    "papermill": {
     "duration": 0.025984,
     "end_time": "2025-10-05T16:16:53.470488",
     "exception": false,
     "start_time": "2025-10-05T16:16:53.444504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Local CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e287974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T16:16:53.523265Z",
     "iopub.status.busy": "2025-10-05T16:16:53.523034Z",
     "iopub.status.idle": "2025-10-05T16:21:51.964633Z",
     "shell.execute_reply": "2025-10-05T16:21:51.963660Z"
    },
    "papermill": {
     "duration": 298.469382,
     "end_time": "2025-10-05T16:21:51.965837",
     "exception": false,
     "start_time": "2025-10-05T16:16:53.496455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 17 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 17 to 34 in group of 5\n",
      "17 , 22 , 27 , 32 , ==========\n",
      "Processing from file 34 to 51 in group of 5\n",
      "34 , 39 , 44 , 49 , ==========\n",
      "Processing from file 51 to 68 in group of 5\n",
      "51 , 56 , 61 , 66 , ==========\n",
      "Processing from file 68 to 85 in group of 5\n",
      "68 , 73 , 78 , 83 , ==========\n",
      "Processing from file 85 to 100 in group of 5\n",
      "85 , 90 , 95 , ==========\n",
      "====================\n",
      "### DISK PART 2\n",
      "Processing from file 0 to 17 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 17 to 34 in group of 5\n",
      "17 , 22 , 27 , 32 , ==========\n",
      "Processing from file 34 to 51 in group of 5\n",
      "34 , 39 , 44 , 49 , ==========\n",
      "Processing from file 51 to 68 in group of 5\n",
      "51 , 56 , 61 , 66 , ==========\n",
      "Processing from file 68 to 85 in group of 5\n",
      "68 , 73 , 78 , 83 , ==========\n",
      "Processing from file 85 to 100 in group of 5\n",
      "85 , 90 , 95 , ==========\n",
      "====================\n",
      "### DISK PART 3\n",
      "Processing from file 0 to 17 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 17 to 34 in group of 5\n",
      "17 , 22 , 27 , 32 , ==========\n",
      "Processing from file 34 to 51 in group of 5\n",
      "34 , 39 , 44 , 49 , ==========\n",
      "Processing from file 51 to 68 in group of 5\n",
      "51 , 56 , 61 , 66 , ==========\n",
      "Processing from file 68 to 85 in group of 5\n",
      "68 , 73 , 78 , 83 , ==========\n",
      "Processing from file 85 to 100 in group of 5\n",
      "85 , 90 , 95 , ==========\n",
      "====================\n",
      "### DISK PART 4\n",
      "Processing from file 0 to 17 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 17 to 34 in group of 5\n",
      "17 , 22 , 27 , 32 , ==========\n",
      "Processing from file 34 to 51 in group of 5\n",
      "34 , 39 , 44 , 49 , ==========\n",
      "Processing from file 51 to 68 in group of 5\n",
      "51 , 56 , 61 , 66 , ==========\n",
      "Processing from file 68 to 85 in group of 5\n",
      "68 , 73 , 78 , 83 , ==========\n",
      "Processing from file 85 to 100 in group of 5\n",
      "85 , 90 , 95 , ==========\n",
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 17 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 17 to 34 in group of 5\n",
      "17 , 22 , 27 , 32 , ==========\n",
      "Processing from file 34 to 51 in group of 5\n",
      "34 , 39 , 44 , 49 , ==========\n",
      "Processing from file 51 to 68 in group of 5\n",
      "51 , 56 , 61 , 66 , ==========\n",
      "Processing from file 68 to 85 in group of 5\n",
      "68 , 73 , 78 , 83 , ==========\n",
      "Processing from file 85 to 100 in group of 5\n",
      "85 , 90 , 95 , ==========\n",
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 17 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 17 to 34 in group of 5\n",
      "17 , 22 , 27 , 32 , ==========\n",
      "Processing from file 34 to 51 in group of 5\n",
      "34 , 39 , 44 , 49 , ==========\n",
      "Processing from file 51 to 68 in group of 5\n",
      "51 , 56 , 61 , 66 , ==========\n",
      "Processing from file 68 to 85 in group of 5\n",
      "68 , 73 , 78 , 83 , ==========\n",
      "Processing from file 85 to 100 in group of 5\n",
      "85 , 90 , 95 , ==========\n",
      "====================\n",
      "### DISK PART 2\n",
      "Processing from file 0 to 17 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 17 to 34 in group of 5\n",
      "17 , 22 , 27 , 32 , ==========\n",
      "Processing from file 34 to 51 in group of 5\n",
      "34 , 39 , 44 , 49 , ==========\n",
      "Processing from file 51 to 68 in group of 5\n",
      "51 , 56 , 61 , 66 , ==========\n",
      "Processing from file 68 to 85 in group of 5\n",
      "68 , 73 , 78 , 83 , ==========\n",
      "Processing from file 85 to 100 in group of 5\n",
      "85 , 90 , 95 , ==========\n",
      "====================\n",
      "### DISK PART 3\n",
      "Processing from file 0 to 17 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 17 to 34 in group of 5\n",
      "17 , 22 , 27 , 32 , ==========\n",
      "Processing from file 34 to 51 in group of 5\n",
      "34 , 39 , 44 , 49 , ==========\n",
      "Processing from file 51 to 68 in group of 5\n",
      "51 , 56 , 61 , 66 , ==========\n",
      "Processing from file 68 to 85 in group of 5\n",
      "68 , 73 , 78 , 83 , ==========\n",
      "Processing from file 85 to 100 in group of 5\n",
      "85 , 90 , 95 , ==========\n",
      "====================\n",
      "### DISK PART 4\n",
      "Processing from file 0 to 17 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 17 to 34 in group of 5\n",
      "17 , 22 , 27 , 32 , ==========\n",
      "Processing from file 34 to 51 in group of 5\n",
      "34 , 39 , 44 , 49 , ==========\n",
      "Processing from file 51 to 68 in group of 5\n",
      "51 , 56 , 61 , 66 , ==========\n",
      "Processing from file 68 to 85 in group of 5\n",
      "68 , 73 , 78 , 83 , ==========\n",
      "Processing from file 85 to 100 in group of 5\n",
      "85 , 90 , 95 , ==========\n",
      "Result: \n",
      "{'num_part': 4, 'matrix_type': 'carts_orders', 'saved_files': ['/kaggle/working/local_cv/train/top_15_carts_orders_0.pqt', '/kaggle/working/local_cv/train/top_15_carts_orders_1.pqt', '/kaggle/working/local_cv/train/top_15_carts_orders_2.pqt', '/kaggle/working/local_cv/train/top_15_carts_orders_3.pqt'], 'top_k': 15}\n",
      "{'num_part': 1, 'matrix_type': 'buy2buy', 'saved_files': ['/kaggle/working/local_cv/train/top_15_buy2buy_0.pqt'], 'top_k': 15}\n",
      "{'num_part': 4, 'matrix_type': 'clicks', 'saved_files': ['/kaggle/working/local_cv/train/top_20_clicks_0.pqt', '/kaggle/working/local_cv/train/top_20_clicks_1.pqt', '/kaggle/working/local_cv/train/top_20_clicks_2.pqt', '/kaggle/working/local_cv/train/top_20_clicks_3.pqt'], 'top_k': 20}\n",
      "CPU times: user 4min 8s, sys: 45.3 s, total: 4min 53s\n",
      "Wall time: 4min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "full_pipeline(\n",
    "    '/kaggle/input/otto-validation/train_parquet/*',\n",
    "    '/kaggle/working/local_cv/train',\n",
    "    carts_orders_params,\n",
    "    buy2buy_params,\n",
    "    clicks_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "065b5ba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T16:21:52.037874Z",
     "iopub.status.busy": "2025-10-05T16:21:52.037670Z",
     "iopub.status.idle": "2025-10-05T16:26:43.276124Z",
     "shell.execute_reply": "2025-10-05T16:26:43.275261Z"
    },
    "papermill": {
     "duration": 291.274997,
     "end_time": "2025-10-05T16:26:43.277454",
     "exception": false,
     "start_time": "2025-10-05T16:21:52.002457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 20 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 20 to 40 in group of 5\n",
      "20 , 25 , 30 , 35 , ==========\n",
      "Processing from file 40 to 60 in group of 5\n",
      "40 , 45 , 50 , 55 , ==========\n",
      "Processing from file 60 to 80 in group of 5\n",
      "60 , 65 , 70 , 75 , ==========\n",
      "Processing from file 80 to 100 in group of 5\n",
      "80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 120 in group of 5\n",
      "100 , 105 , 110 , 115 , ==========\n",
      "====================\n",
      "### DISK PART 2\n",
      "Processing from file 0 to 20 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 20 to 40 in group of 5\n",
      "20 , 25 , 30 , 35 , ==========\n",
      "Processing from file 40 to 60 in group of 5\n",
      "40 , 45 , 50 , 55 , ==========\n",
      "Processing from file 60 to 80 in group of 5\n",
      "60 , 65 , 70 , 75 , ==========\n",
      "Processing from file 80 to 100 in group of 5\n",
      "80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 120 in group of 5\n",
      "100 , 105 , 110 , 115 , ==========\n",
      "====================\n",
      "### DISK PART 3\n",
      "Processing from file 0 to 20 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 20 to 40 in group of 5\n",
      "20 , 25 , 30 , 35 , ==========\n",
      "Processing from file 40 to 60 in group of 5\n",
      "40 , 45 , 50 , 55 , ==========\n",
      "Processing from file 60 to 80 in group of 5\n",
      "60 , 65 , 70 , 75 , ==========\n",
      "Processing from file 80 to 100 in group of 5\n",
      "80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 120 in group of 5\n",
      "100 , 105 , 110 , 115 , ==========\n",
      "====================\n",
      "### DISK PART 4\n",
      "Processing from file 0 to 20 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 20 to 40 in group of 5\n",
      "20 , 25 , 30 , 35 , ==========\n",
      "Processing from file 40 to 60 in group of 5\n",
      "40 , 45 , 50 , 55 , ==========\n",
      "Processing from file 60 to 80 in group of 5\n",
      "60 , 65 , 70 , 75 , ==========\n",
      "Processing from file 80 to 100 in group of 5\n",
      "80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 120 in group of 5\n",
      "100 , 105 , 110 , 115 , ==========\n",
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 20 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 20 to 40 in group of 5\n",
      "20 , 25 , 30 , 35 , ==========\n",
      "Processing from file 40 to 60 in group of 5\n",
      "40 , 45 , 50 , 55 , ==========\n",
      "Processing from file 60 to 80 in group of 5\n",
      "60 , 65 , 70 , 75 , ==========\n",
      "Processing from file 80 to 100 in group of 5\n",
      "80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 120 in group of 5\n",
      "100 , 105 , 110 , 115 , ==========\n",
      "====================\n",
      "### DISK PART 1\n",
      "Processing from file 0 to 20 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 20 to 40 in group of 5\n",
      "20 , 25 , 30 , 35 , ==========\n",
      "Processing from file 40 to 60 in group of 5\n",
      "40 , 45 , 50 , 55 , ==========\n",
      "Processing from file 60 to 80 in group of 5\n",
      "60 , 65 , 70 , 75 , ==========\n",
      "Processing from file 80 to 100 in group of 5\n",
      "80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 120 in group of 5\n",
      "100 , 105 , 110 , 115 , ==========\n",
      "====================\n",
      "### DISK PART 2\n",
      "Processing from file 0 to 20 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 20 to 40 in group of 5\n",
      "20 , 25 , 30 , 35 , ==========\n",
      "Processing from file 40 to 60 in group of 5\n",
      "40 , 45 , 50 , 55 , ==========\n",
      "Processing from file 60 to 80 in group of 5\n",
      "60 , 65 , 70 , 75 , ==========\n",
      "Processing from file 80 to 100 in group of 5\n",
      "80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 120 in group of 5\n",
      "100 , 105 , 110 , 115 , ==========\n",
      "====================\n",
      "### DISK PART 3\n",
      "Processing from file 0 to 20 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 20 to 40 in group of 5\n",
      "20 , 25 , 30 , 35 , ==========\n",
      "Processing from file 40 to 60 in group of 5\n",
      "40 , 45 , 50 , 55 , ==========\n",
      "Processing from file 60 to 80 in group of 5\n",
      "60 , 65 , 70 , 75 , ==========\n",
      "Processing from file 80 to 100 in group of 5\n",
      "80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 120 in group of 5\n",
      "100 , 105 , 110 , 115 , ==========\n",
      "====================\n",
      "### DISK PART 4\n",
      "Processing from file 0 to 20 in group of 5\n",
      "0 , 5 , 10 , 15 , ==========\n",
      "Processing from file 20 to 40 in group of 5\n",
      "20 , 25 , 30 , 35 , ==========\n",
      "Processing from file 40 to 60 in group of 5\n",
      "40 , 45 , 50 , 55 , ==========\n",
      "Processing from file 60 to 80 in group of 5\n",
      "60 , 65 , 70 , 75 , ==========\n",
      "Processing from file 80 to 100 in group of 5\n",
      "80 , 85 , 90 , 95 , ==========\n",
      "Processing from file 100 to 120 in group of 5\n",
      "100 , 105 , 110 , 115 , ==========\n",
      "Result: \n",
      "{'num_part': 4, 'matrix_type': 'carts_orders', 'saved_files': ['/kaggle/working/local_cv/train/top_15_carts_orders_0.pqt', '/kaggle/working/local_cv/train/top_15_carts_orders_1.pqt', '/kaggle/working/local_cv/train/top_15_carts_orders_2.pqt', '/kaggle/working/local_cv/train/top_15_carts_orders_3.pqt'], 'top_k': 15}\n",
      "{'num_part': 1, 'matrix_type': 'buy2buy', 'saved_files': ['/kaggle/working/local_cv/train/top_15_buy2buy_0.pqt'], 'top_k': 15}\n",
      "{'num_part': 4, 'matrix_type': 'clicks', 'saved_files': ['/kaggle/working/local_cv/train/top_20_clicks_0.pqt', '/kaggle/working/local_cv/train/top_20_clicks_1.pqt', '/kaggle/working/local_cv/train/top_20_clicks_2.pqt', '/kaggle/working/local_cv/train/top_20_clicks_3.pqt'], 'top_k': 20}\n",
      "CPU times: user 4min 12s, sys: 45.6 s, total: 4min 58s\n",
      "Wall time: 4min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# All\n",
    "full_pipeline(\n",
    "    '/kaggle/input/otto-validation/*_parquet/*',\n",
    "    '/kaggle/working/local_cv/train',\n",
    "    carts_orders_params,\n",
    "    buy2buy_params,\n",
    "    clicks_params\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 4493939,
     "sourceId": 38760,
     "sourceType": "competition"
    },
    {
     "datasetId": 2597726,
     "sourceId": 4436180,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2623568,
     "sourceId": 4483558,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8384420,
     "sourceId": 13227527,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1726.837925,
   "end_time": "2025-10-05T16:26:44.342793",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-05T15:57:57.504868",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
